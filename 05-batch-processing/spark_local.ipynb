{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from dotenv import load_dotenv, find_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_dotenv()\n",
    "load_dotenv()\n",
    "\n",
    "google_credentials = os.environ.get('GOOGLE_APPLICATION_CREDENTIALS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_config = [\n",
    "    (\"spark.jars\", \"https://storage.googleapis.com/hadoop-lib/gcs/gcs-connector-hadoop3-latest.jar\"),\n",
    "    (\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\"),\n",
    "    (\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", google_credentials),\n",
    "    (\"spark.sql.adaptive.enabled\", \"false\"),\n",
    "]\n",
    "\n",
    "spark_conf = SparkConf().setAll(spark_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/02/25 13:54:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "        .appName('spark_dezoomcamp') \\\n",
    "        .master('local[*]') \\\n",
    "        .config(conf=spark_conf) \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_green = spark.read\\\n",
    "    .parquet('gs://de-zoomcamp-bq-1/green_taxi/green_tripdata_2019-01.parquet')\n",
    "df_green.createOrReplaceTempView('green_taxi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_yellow = spark.read\\\n",
    "    .parquet('gs://de-zoomcamp-bq-1/yellow_taxi/yellow_tripdata_2019-01.parquet')\n",
    "df_yellow.createOrReplaceTempView('yellow_taxi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f\"\"\"\n",
    "    SELECT \n",
    "        PULocationID AS zone, \n",
    "        date_trunc('HOUR', lpep_pickup_datetime) AS pickup_datetime,\n",
    "        DECIMAL(SUM(total_amount)) AS green_revenue,\n",
    "        COUNT(1) AS green_number_of_records\n",
    "    FROM \n",
    "        green_taxi\n",
    "    WHERE lpep_pickup_datetime > '2019-01-01 00:00:00'\n",
    "    GROUP BY \n",
    "        zone, pickup_datetime\n",
    "\"\"\"\n",
    "\n",
    "df_green_tmp = spark.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f\"\"\"\n",
    "    SELECT \n",
    "        PULocationID AS zone, \n",
    "        date_trunc('HOUR', tpep_pickup_datetime) AS pickup_datetime,\n",
    "        DECIMAL(SUM(total_amount)) AS yellow_revenue,\n",
    "        COUNT(1) AS yellow_number_of_records\n",
    "    FROM \n",
    "        yellow_taxi\n",
    "    WHERE tpep_pickup_datetime > '2019-01-01 00:00:00'\n",
    "    GROUP BY \n",
    "        zone, pickup_datetime\n",
    "\"\"\"\n",
    "\n",
    "df_yellow_tmp = spark.sql(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_join = df_green_tmp.join(df_yellow_tmp, on=['zone', 'pickup_datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:============================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------+-------------+-----------------------+--------------+------------------------+\n",
      "|zone|    pickup_datetime|green_revenue|green_number_of_records|yellow_revenue|yellow_number_of_records|\n",
      "+----+-------------------+-------------+-----------------------+--------------+------------------------+\n",
      "|  55|2019-01-01 13:00:00|          286|                      5|           156|                       3|\n",
      "|  43|2019-01-01 16:00:00|           26|                      3|          3300|                     257|\n",
      "|  33|2019-01-01 20:00:00|          155|                     11|            65|                       3|\n",
      "| 244|2019-01-01 21:00:00|           96|                      7|           188|                       8|\n",
      "|  39|2019-01-02 08:00:00|          533|                     15|            45|                       2|\n",
      "|  37|2019-01-02 09:00:00|           95|                      4|            31|                       1|\n",
      "| 236|2019-01-02 11:00:00|           44|                      4|          5399|                     497|\n",
      "| 129|2019-01-02 10:00:00|          282|                     20|            37|                       2|\n",
      "|   7|2019-01-02 12:00:00|          383|                     25|           261|                      19|\n",
      "|  51|2019-01-02 12:00:00|          160|                      4|            10|                       1|\n",
      "|  76|2019-01-02 13:00:00|          256|                     11|           216|                       5|\n",
      "| 255|2019-01-02 17:00:00|          135|                      8|            56|                       3|\n",
      "|  76|2019-01-02 20:00:00|          476|                      9|            62|                       1|\n",
      "| 116|2019-01-03 00:00:00|           14|                      1|           108|                       9|\n",
      "| 129|2019-01-03 00:00:00|           81|                     10|           123|                       6|\n",
      "| 255|2019-01-03 02:00:00|           56|                      6|            24|                       1|\n",
      "| 146|2019-01-03 07:00:00|           10|                      1|           251|                      13|\n",
      "| 225|2019-01-03 07:00:00|          309|                     12|            22|                       1|\n",
      "| 191|2019-01-03 08:00:00|          174|                      5|           190|                       3|\n",
      "| 173|2019-01-03 08:00:00|           21|                      3|            43|                       1|\n",
      "+----+-------------------+-------------+-----------------------+--------------+------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_join.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
